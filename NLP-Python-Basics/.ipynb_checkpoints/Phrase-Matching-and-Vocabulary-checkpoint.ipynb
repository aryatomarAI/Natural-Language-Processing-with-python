{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58c3260",
   "metadata": {},
   "source": [
    "# Maching and Vocabulary\n",
    "So far we've seen how a body of text is divided into tokens, and how individual tokens are parsed and tagged with the parts of sppech, dependencies and lemmas.\n",
    "\n",
    "In this notebook we will identify and label specific phrases that match the patterns we can define ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8bfb9e",
   "metadata": {},
   "source": [
    "## Rule based matching\n",
    "spaCy offers a rule-matching tool called Matcher that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations and you can add multiple patterns to the same matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189fcb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp=en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "364b5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matcher library\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# create a Matcher object\n",
    "matcher=Matcher(nlp.vocab,validate=True)\n",
    "\n",
    "# Create a doc text\n",
    "doc=nlp(u\"The Solar Power industry continues to grow as solarpower increases. Solar-Power is amazing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b388b6",
   "metadata": {},
   "source": [
    "## creating patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "854c2df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create here patterns to find 3 matches in the doc 1. solar power, 2. Solarpower, 3. Solar-Power\n",
    "pattern1=[{\"LOWER\":'solarpower'}]\n",
    "pattern2=[{\"LOWER\":'solar'},{\"LOWER\":\"power\"}]\n",
    "pattern3=[{\"LOWER\":'solar'},{\"IS_PUNCT\":True},{\"LOWER\":\"power\"}]\n",
    "\n",
    "matcher.add('SolarPower',[pattern1,pattern2,pattern3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697143be",
   "metadata": {},
   "source": [
    "Let's break this down:\n",
    "* `pattern1` looks for a single token whose lowercase text reads 'solarpower'\n",
    "* `pattern2` looks for two adjacent tokens that read 'solar' and 'power' in that order\n",
    "* `pattern3` looks for three adjacent tokens, with a middle token that can be any punctuation.<font color=green>*</font>\n",
    "\n",
    "<font color=green>\\* Remember that single spaces are not tokenized, so they don't count as punctuation.</font>\n",
    "<br>Once we define our patterns, we pass them into `matcher` with the name 'SolarPower', and set *callbacks* to `None` (more on callbacks later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78c699",
   "metadata": {},
   "source": [
    "## Appling the matcher to the doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40fd6246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The Solar Power industry continues to grow as solarpower increases. Solar-Power is amazing."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23892f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_matches=matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d07eeec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 8, 9), (8656102463236116519, 11, 14)]\n"
     ]
    }
   ],
   "source": [
    "print(find_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48d436",
   "metadata": {},
   "source": [
    "matcher returns a list of tuples. Each tuple contains an ID for the match, with start & end tokens that map to the span doc[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a9a2ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "8656102463236116519 SolarPower 8 9 solarpower\n",
      "8656102463236116519 SolarPower 11 14 Solar-Power\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in find_matches:\n",
    "    string_id=nlp.vocab.strings[match_id]\n",
    "    span=doc[start:end]\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a94a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.remove(\"SolarPower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ccc105",
   "metadata": {},
   "source": [
    "### Setting pattern options and quantifiers\n",
    "You can make token rules optional by passing an `'OP':'*'` argument. This lets us streamline our patterns list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "475c6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1=[{\"LOWER\":'solarpower'}]\n",
    "pattern2=[{\"LOWER\":'solar'},{\"IS_PUNCT\":True,\"OP\":\"*\"},{\"LOWER\":'power'}]\n",
    "matcher.add('SolarPower',[pattern1,pattern2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "911d2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=nlp(u\"Solar---Power is solarpower\")\n",
    "find_matches=matcher(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "015836be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8656102463236116519, 0, 3), (8656102463236116519, 4, 5)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "689daca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 0 3 Solar---Power\n",
      "8656102463236116519 SolarPower 4 5 solarpower\n"
     ]
    }
   ],
   "source": [
    "for match_id,start,end in find_matches:\n",
    "    string_id=nlp.vocab.strings[match_id]\n",
    "    span=doc1[start:end]\n",
    "    print(match_id,string_id,start,end,span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df21c4",
   "metadata": {},
   "source": [
    "This found both two-word patterns, with and without the hyphen!\n",
    "\n",
    "The following quantifiers can be passed to the `'OP'` key:\n",
    "<table><tr><th>OP</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
    "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
    "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
    "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45034718",
   "metadata": {},
   "source": [
    "### Be careful with lemmas!\n",
    "If we wanted to match on both 'solar power' and 'solar powered', it might be tempting to look for the *lemma* of 'powered' and expect it to be 'power'. This is not always the case! The lemma of the *adjective* 'powered' is still 'powered':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "759b7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.remove('SolarPower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e891102",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1=[{\"LOWER\":\"solarpower\"}]\n",
    "pattern2=[{\"LOWER\":'solar'},{\"IS_PUNCT\":True,\"OP\":\"*\"},{\"LEMMA\":'power'}]\n",
    "pattern3=[{\"LOWER\":'solarpowered'}]\n",
    "pattern4=[{\"LOWER\":'solar'},{\"IS_PUNCT\":True,\"OP\":\"*\"},{\"LEMMA\":'powered'}]\n",
    "\n",
    "matcher.add('SolarPolar',[pattern1,pattern2,pattern3,pattern4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4569d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2=nlp(u\"solar-powered energy runs solar-powered cars\")\n",
    "\n",
    "find_matches=matcher(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9f4d17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6168938391642580357, 0, 3), (6168938391642580357, 5, 8)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "355122eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6168938391642580357 SolarPolar 0 3 solar-powered\n",
      "6168938391642580357 SolarPolar 5 8 solar-powered\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in find_matches:\n",
    "    string_id=nlp.vocab.strings[match_id]\n",
    "    span=doc2[start:end]\n",
    "    print(match_id,string_id,start,end,span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d94b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.remove(\"SolarPolar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676f064",
   "metadata": {},
   "source": [
    "## Other token attributes\n",
    "Besides lemmas, there are a variety of token attributes we can use to determine matching rules:\n",
    "<table><tr><th>Attribute</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\n",
    "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\n",
    "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\n",
    "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\n",
    "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\n",
    "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\n",
    "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\n",
    "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\n",
    "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559344b",
   "metadata": {},
   "source": [
    "___\n",
    "## PhraseMatcher\n",
    "In the above section we used token patterns to perform rule-based matching. An alternative - and often more efficient - method is to match on terminology lists. In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into `matcher` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a6227f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import phrase Matcher and make an object of it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher=PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c1d9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../TextFiles/reaganomics.txt\") as f:\n",
    "    doc3=nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bdefa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a list of match phrases:\n",
    "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']\n",
    "\n",
    "# Next, convert each phrase to a Doc object:\n",
    "phrase_patterns=[nlp(text) for text in phrase_list]\n",
    "\n",
    "# Pass each Doc object into matcher\n",
    "matcher.add(\"EconMatcher\",[*phrase_patterns])\n",
    "\n",
    "found_matches=matcher(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39bbc391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3680293220734633682, 41, 45),\n",
       " (3680293220734633682, 49, 53),\n",
       " (3680293220734633682, 54, 56),\n",
       " (3680293220734633682, 61, 65),\n",
       " (3680293220734633682, 673, 677),\n",
       " (3680293220734633682, 2987, 2991)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c020a97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680293220734633682 EconMatcher 41 45 supply-side economics\n",
      "3680293220734633682 EconMatcher 49 53 trickle-down economics\n",
      "3680293220734633682 EconMatcher 54 56 voodoo economics\n",
      "3680293220734633682 EconMatcher 61 65 free-market economics\n",
      "3680293220734633682 EconMatcher 673 677 supply-side economics\n",
      "3680293220734633682 EconMatcher 2987 2991 trickle-down economics\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id=nlp.vocab.strings[match_id]\n",
    "    span=doc3[start:end]\n",
    "    print(match_id,string_id,start,end,span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "035ec506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680293220734633682 EconMatcher 41 45 policies are commonly associated with supply-side economics, referred to as trickle-down economics\n",
      "3680293220734633682 EconMatcher 49 53 economics, referred to as trickle-down economics or voodoo economics by political opponents, and\n",
      "3680293220734633682 EconMatcher 54 56 trickle-down economics or voodoo economics by political opponents, and free-market\n",
      "3680293220734633682 EconMatcher 61 65 by political opponents, and free-market economics by political advocates.\n",
      "\n",
      "The four pillars\n",
      "3680293220734633682 EconMatcher 673 677 attracted a following from the supply-side economics movement, which formed in opposition to Keynesian\n",
      "3680293220734633682 EconMatcher 2987 2991 became widely known as \"trickle-down economics\", due to the significant cuts in\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id=nlp.vocab.strings[match_id]\n",
    "    span=doc3[start-5:end+8]\n",
    "    print(match_id,string_id,start,end,span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de124d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
