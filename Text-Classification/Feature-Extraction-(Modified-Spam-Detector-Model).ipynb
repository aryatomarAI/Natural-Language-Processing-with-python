{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a97b8a",
   "metadata": {},
   "source": [
    "# Feature Extraction from text\n",
    "Most Classic Machine learning models or algorithms can't take in raw text as an input. Instead we need to perform a feature extraction from the raw text in order to pass numerical features to the machine learning algorithms. For example we can count the occurence of each word to map text to a number. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf919c4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Feature Extraction using Count Vectorization along with term frequency and Inverse Document frequency**\n",
    "\n",
    "Let's see how to import these tools using scikit-learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a568cf",
   "metadata": {},
   "source": [
    "### Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e73be26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message=[\"Hey, let's watch the game today of lakers, bring some snacks with you!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4221072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1fe88",
   "metadata": {},
   "source": [
    "An alternative to Count Vectorizer is something called TfidfVectorizer. It also creates a document term matrix from our messages.\n",
    "\n",
    "However, instead of filling the DTM with token counts it calculates term frequency-inverse document frequency value for each word(Tf-IDF).\n",
    "\n",
    "**Term Frequency tf(t,d):** is the raw count of a term in a document, i.e. the number of times that term t occurs in document d.\n",
    "    \n",
    "However term frequency alone is not enugh for a through feature analysis of the text. Let's imagine very common terms like \"a\" or \"the\". Because the term \"the\" so common, term frequency will tend to incorrectly emphasize documents which happen to use the word \"the\" more frequently, without giving the weight to more important words like \"red\" or \"clouds\".\n",
    "\n",
    "For this reason we use Inverse document frequency\n",
    "\n",
    "**An Inverse document frequency factor is incorporated which diminishes the weight of terms that occurs very frequently in the document set and increases the weight of terms that occur rarely.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e881b",
   "metadata": {},
   "source": [
    "### Import Term Frequency-Inverse Document Frequency factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "607201f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect=TfidfVectorizer()\n",
    "dtm=vect.fit_transform(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa14d37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x13 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecef8f9",
   "metadata": {},
   "source": [
    "### Building Natural Language Processor  from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ce76401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile 1.txt\n",
    "This is a story about cats\n",
    "Cats are furry animals\n",
    "Cats are super mean pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "feaf9bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.txt\n",
    "This story is about dogs\n",
    "Dogs are super energetic and like to play all the time\n",
    "Dogs are very faithful to humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "62d9cbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 1,\n",
       " 'is': 2,\n",
       " 'a': 3,\n",
       " 'story': 4,\n",
       " 'about': 5,\n",
       " 'cats': 6,\n",
       " 'are': 7,\n",
       " 'furry': 8,\n",
       " 'animals': 9,\n",
       " 'super': 10,\n",
       " 'mean': 11,\n",
       " 'pets': 12}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's code a script that will count words occurence in 1.txt\n",
    "vocab={}\n",
    "i=1\n",
    "with open('1.txt') as f:\n",
    "    x=f.read().lower().split()\n",
    "    \n",
    "for word in x:\n",
    "    if word in vocab:\n",
    "        continue\n",
    "    else:\n",
    "        vocab[word]=i\n",
    "        i+=1\n",
    "vocab    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "972476cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 1,\n",
       " 'is': 2,\n",
       " 'a': 3,\n",
       " 'story': 4,\n",
       " 'about': 5,\n",
       " 'cats': 6,\n",
       " 'are': 7,\n",
       " 'furry': 8,\n",
       " 'animals': 9,\n",
       " 'super': 10,\n",
       " 'mean': 11,\n",
       " 'pets': 12,\n",
       " 'dogs': 13,\n",
       " 'energetic': 14,\n",
       " 'and': 15,\n",
       " 'like': 16,\n",
       " 'to': 17,\n",
       " 'play': 18,\n",
       " 'all': 19,\n",
       " 'the': 20,\n",
       " 'time': 21,\n",
       " 'very': 22,\n",
       " 'faithful': 23,\n",
       " 'humans': 24}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's code a script that will count words occurence in 2.txt\n",
    "with open('2.txt') as f:\n",
    "    y=f.read().lower().split()\n",
    "    \n",
    "for word in y:\n",
    "    if word in vocab:\n",
    "        continue\n",
    "    else:\n",
    "        vocab[word]=i\n",
    "        i+=1\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f681413",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Now that we've encapsulated our \"entire language\" in a dictionary, let's perform feature extraction on each of our original documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "978bbca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.txt',\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty vector with space for each word in the vocab\n",
    "vect=[\"1.txt\"]+[0]*len(vocab)\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19bdd53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0df412c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.txt',\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map the frequencies of each word in 1.txt to our vector\n",
    "with open(\"1.txt\") as f:\n",
    "    x=f.read().lower().split()\n",
    "    \n",
    "for word in x:\n",
    "    vect[vocab[word]] +=1\n",
    "        \n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9f5b1221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.txt',\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map the frequencies of each word in 2.txt to our vector\n",
    "vect2=['2.txt']+[0]*len(vocab)\n",
    "\n",
    "with open('2.txt') as f:\n",
    "    x=f.read().lower().split()\n",
    "for word in x:\n",
    "    vect2[vocab[word]]+=1\n",
    "    \n",
    "vect2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e3bf35e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.txt', 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['2.txt', 1, 1, 0, 1, 1, 0, 2, 0, 0, 1, 0, 0, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# compare two vectors\n",
    "print(f\"{vect}\\n{vect2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fadc29f",
   "metadata": {},
   "source": [
    "By comparing the vectors we see that some words are common to both, some appear only in 1.txt, others only in 2.txt. Extending this logic to tens of thousands of documents, we would see the vocabulary dictionary grow to hundreds of thousands of words. Vectors would contain mostly zero values, making them sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab04bca2",
   "metadata": {},
   "source": [
    "## Bag of Words and Tf-idf\n",
    "In the above examples, each vector can be considered a *bag of words*. By itself these may not be helpful until we consider *term frequencies*, or how often individual words appear in documents. A simple way to calculate term frequencies is to divide the number of occurrences of a word by the total number of words in the document. In this way, the number of times a word appears in large documents can be compared to that of smaller documents.\n",
    "\n",
    "However, it may be hard to differentiate documents based on term frequency if a word shows up in a majority of documents. To handle this we also consider *inverse document frequency*, which is the total number of documents divided by the number of documents that contain the word. In practice we convert this value to a logarithmic scale, as described [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency).\n",
    "\n",
    "Together these terms become [**tf-idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324032ee",
   "metadata": {},
   "source": [
    "## Tokenization and Tagging\n",
    "When we created our vectors the first thing we did was split the incoming text on whitespace with `.split()`. This was a crude form of *tokenization* - that is, dividing a document into individual words. In this simple example we didn't worry about punctuation or different parts of speech. In the real world we rely on some fairly sophisticated *morphology* to parse text appropriately.\n",
    "\n",
    "Once the text is divided, we can go back and *tag* our tokens with information about parts of speech, grammatical dependencies, etc. This adds more dimensions to our data and enables a deeper understanding of the context of specific documents. For this reason, vectors become ***high dimensional sparse matrices***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8daf76",
   "metadata": {},
   "source": [
    "## Feature Extraction From Text(Spam Detector Model)\n",
    "In scikit-Laerm-Primer notebook our spam detector model's performance was really bad so this time we will use our message feature as an input and see how our model works then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "00c00832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\envs\\nlp_course\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\DELL\\Anaconda3\\envs\\nlp_course\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Import all the tools\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1bf82e",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c06276ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../TextFiles/smsspamcollection.tsv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ea81101a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  punct\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111      9\n",
       "1   ham                      Ok lar... Joking wif u oni...      29      6\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      6\n",
       "3   ham  U dun say so early hor... U c already then say...      49      6\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61      2"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2003b841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572.000000</td>\n",
       "      <td>5572.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>80.489950</td>\n",
       "      <td>4.177495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>59.942907</td>\n",
       "      <td>4.623919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>122.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>910.000000</td>\n",
       "      <td>133.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            length        punct\n",
       "count  5572.000000  5572.000000\n",
       "mean     80.489950     4.177495\n",
       "std      59.942907     4.623919\n",
       "min       2.000000     0.000000\n",
       "25%      36.000000     2.000000\n",
       "50%      62.000000     3.000000\n",
       "75%     122.000000     6.000000\n",
       "max     910.000000   133.000000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ddd74f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "message    0\n",
       "length     0\n",
       "punct      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check for missing values in the data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfcf504",
   "metadata": {},
   "source": [
    "## Split the data into X and y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "445f9422",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['message']\n",
    "y=df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6f7dba",
   "metadata": {},
   "source": [
    "## Split the data into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d05c9dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fd438f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7a8c344e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1393,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05720ab1",
   "metadata": {},
   "source": [
    "## Scikit-learn's CountVectorizer\n",
    "Text preprocessing, tokenizing and the ability to filter out stopwords are all included in [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), which builds a dictionary of features and transforms documents to feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b5b71450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7490 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 55879 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's convert our text to numerical vectors using CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect=CountVectorizer()\n",
    "X_train_counts=count_vect.fit_transform(X_train)\n",
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9aa722fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 7490)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c045f",
   "metadata": {},
   "source": [
    "## Transform Counts to Frequencies with Tf-idf\n",
    "While counting words is helpful, longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid this we can simply divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called **tf** for Term Frequencies.\n",
    "\n",
    "Another refinement on top of **tf** is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called **tf–idf** for “Term Frequency times Inverse Document Frequency”.\n",
    "\n",
    "Both tf and tf–idf can be computed as follows using [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "76b2a272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 7490)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer=TfidfTransformer()\n",
    "X_train_trans=transformer.fit_transform(X_train_counts)\n",
    "X_train_trans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846dd71d",
   "metadata": {},
   "source": [
    "## Combine Steps with TfidVectorizer\n",
    "We can combine the CountVectorizer and TfidTransformer steps into one using [TfidVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6a104389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 7490)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect=TfidfVectorizer()\n",
    "X_train_tfidf=vect.fit_transform(X_train)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b636a",
   "metadata": {},
   "source": [
    "## Choose a model to fit this data on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "55e49195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model=LinearSVC()\n",
    "\n",
    "model.fit(X_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e38372",
   "metadata": {},
   "source": [
    "## Build a Pipeline\n",
    "Remember that only our training set has been vectorized into a full vocabulary. In order to perform an analysis on our test set we'll have to submit it to the same procedures. Fortunately scikit-learn offers a [**Pipeline**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class that behaves like a compound classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bb312aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf=Pipeline([('tfidf',TfidfVectorizer()),('clf',LinearSVC())])\n",
    "\n",
    "text_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4bdcbd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using model\n",
    "\n",
    "predictions=text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c127238e",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8dc08d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.990667623833453"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "26b8fde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1205,    2],\n",
       "       [  11,  175]], dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7afa4a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>1205</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>11</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ham  spam\n",
       "ham   1205     2\n",
       "spam    11   175"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(confusion_matrix(y_test,predictions),index=[\"ham\",\"spam\"],columns=[\"ham\",\"spam\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6d663023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99      1207\n",
      "        spam       0.99      0.94      0.96       186\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1393\n",
      "   macro avg       0.99      0.97      0.98      1393\n",
      "weighted avg       0.99      0.99      0.99      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863a396",
   "metadata": {},
   "source": [
    "## Let's try our model on custom messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a2253834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], dtype=object)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"Congratulations, You have won a brand new car in a jackpot click on the link to claim the car, Hurry Up!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bdbd84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
